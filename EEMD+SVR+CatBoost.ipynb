{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920b2570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyEMD import EEMD\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from catboost import Pool, CatBoostRegressor\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f90029a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(fileName: str) -> object:\n",
    "    return pd.read_csv(os.getcwd() + fileName +\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe38e248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_eemd(nIMFs: object, eIMFs: object, df: object) -> None:\n",
    "    plt.figure(figsize=(15,17))\n",
    "    plt.subplot(nIMFs+1, 1, 1)\n",
    "    plt.plot(df[\"grid1-loss\"].values, 'r')\n",
    "    \n",
    "    for n in range(nIMFs):\n",
    "        plt.subplot(nIMFs+1, 1, n+2)\n",
    "        plt.plot(eIMFs[n], 'g')\n",
    "        plt.ylabel(\"eIMF %i\" %(n+1))\n",
    "        plt.locator_params(axis='y', nbins=5)\n",
    "        \n",
    "    plt.xlabel(\"Time [H]\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f84cae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(y_test: [float], y_pred: [float]) -> None:\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(y_test,  label='True Data')\n",
    "    plt.plot(y_pred,  label='Prediction')\n",
    "    plt.xlabel(\"Time [H]\")\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf64b6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_eemd(df: object, detection: str) -> object:\n",
    "    num_trials = 12\n",
    "    # Create an EEMD instance\n",
    "    eemd = EEMD(trials=num_trials)\n",
    "\n",
    "    # Perform EEMD decomposition\n",
    "    return eemd.eemd(df[\"grid1-loss\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cb990f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_data(\"dataset_all_ewma_curve_fs\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1e4deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'datetime' column to datetime objects\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df.sort_values(by='datetime', inplace=True)\n",
    "\n",
    "reference_date = pd.to_datetime(\"2019-11-30\")\n",
    "\n",
    "window_size_days = 180  # 180 days for training\n",
    "step_days = 1  # 1 day for sliding window\n",
    "\n",
    "# Convert window size and step to hourly intervals\n",
    "window_size_hours = window_size_days * 24\n",
    "step_hours = step_days * 24\n",
    "\n",
    "# List to store the indices of the training and testing data\n",
    "train_indices = []\n",
    "test_indices = []\n",
    "\n",
    "ttrain_indices = list()\n",
    "ttest_indices = list()\n",
    "\n",
    "# Select 181 days (180 days for training and 1 day for testing), sliding window for testing\n",
    "for i in range(window_size_hours, len(df) - step_hours, step_hours):\n",
    "    # Convert the current timestamp to the same timezone as reference_date\n",
    "    current_timestamp = df['datetime'][i].tz_localize(reference_date.tzinfo)\n",
    "    \n",
    "    if current_timestamp >= reference_date:\n",
    "        # Add indices for training data\n",
    "        ttrain_indices.append(range(i - window_size_hours, i))\n",
    "        # Add indices for testing data\n",
    "        ttest_indices.append(range(i, i + step_hours))\n",
    "    else:\n",
    "        # Add indices for training data\n",
    "        train_indices.append(range(i - window_size_hours, i))\n",
    "        # Add indices for testing data\n",
    "        test_indices.append(range(i, i  + step_hours))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd078d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_archi = [\n",
    "    [6, {'C': 1.0, 'gamma': 0.01, 'kernel': 'rbf'}],\n",
    "    [7, {'C': 1.0, 'gamma': 0.01, 'kernel': 'rbf'}],\n",
    "    [8, {'C': 1.0, 'gamma': 0.01, 'kernel': 'rbf'}],\n",
    "    [9, {'C': 1.0, 'gamma': 0.01, 'kernel': 'rbf'}],\n",
    "    [10, {'C': 1.0, 'gamma': 0.01, 'kernel': 'rbf'}],\n",
    "    [11, {'C': 1.0, 'gamma': 0.01, 'kernel': 'rbf'}],\n",
    "    #[12, {'C': 1.0, 'gamma': 0.01, 'kernel': 'rbf'}]\n",
    "]\n",
    "\n",
    "error_list_v1 = [[] for _ in range(len(svr_archi))]\n",
    "true_values_v1 = [[] for _ in range(len(svr_archi))]\n",
    "svr_train_models = list()\n",
    "\n",
    "imfs = [0, 1, 2, 3, 4, 5]\n",
    "pred_cat_list = [[] for _ in range(len(imfs))]\n",
    "true_cat_list = [[] for _ in range(len(imfs))]\n",
    "trained_models = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a7d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_eemd(grid_loss, adder):\n",
    "    return mean_absolute_percentage_error(grid_loss, adder)*100\n",
    "\n",
    "def add_up(decomposition, k):\n",
    "    adder = list()\n",
    "    for i in range(len(decomposition[0])):\n",
    "        sumator = 0\n",
    "        for j in range(len(decomposition) - k):\n",
    "            sumator += decomposition[j][i]\n",
    "        adder.append(sumator)\n",
    "    return adder\n",
    "\n",
    "mini = 100\n",
    "best_k = None\n",
    "best_decomp = None\n",
    "\n",
    "for i in range(3):\n",
    "    decomposition = make_eemd(df[:test_indices[-1][-1]+1], \"parabolic\")\n",
    "    for k in range(3):\n",
    "        adder = add_up(decomposition, k)\n",
    "        mape = evaluation_eemd(df[\"grid1-loss\"][:test_indices[-1][-1]+1], adder)\n",
    "        if mape < mini:\n",
    "            mini = mape\n",
    "            best_k = k\n",
    "            best_decomp = decomposition\n",
    "            \n",
    "add_up_decom = add_up(best_decomp, best_k)\n",
    "    \n",
    "plt.plot(df[\"grid1-loss\"][:test_indices[-1][-1]+1], label = \"true\")\n",
    "plt.plot(add_up_decom, label = \"IMFs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e8e611",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx, archi in enumerate(svr_archi):\n",
    "    print(f\"SVR Architecture {idx}: {archi}\")\n",
    "    C = archi[1][\"C\"]\n",
    "    gamma = archi[1][\"gamma\"]\n",
    "    kernel = archi[1][\"kernel\"]\n",
    "    model = SVR(C = C, gamma = gamma, kernel = kernel)\n",
    "\n",
    "    for j in range(len(train_indices)):\n",
    "        df_indexes = list(train_indices[j]) + list(test_indices[j])\n",
    "        print(df_indexes[0], df_indexes[-1])\n",
    "\n",
    "        train_df = copy.deepcopy(df.iloc[train_indices[j]]).drop([\"datetime\", \"grid1-loss\", \"has incorrect data\"], axis= 1)\n",
    "        test_df = copy.deepcopy(df.iloc[test_indices[j]]).drop([\"datetime\", \"grid1-loss\", \"has incorrect data\"], axis= 1)\n",
    "\n",
    "        y_train = best_decomp[archi[0]][train_indices[j]]\n",
    "        y_test = best_decomp[archi[0]][test_indices[j]]\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(train_df.values)\n",
    "        X_test_scaled = scaler.transform(test_df.values)\n",
    "\n",
    "        # Train the best model on the entire training data\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_train_pred = model.predict(X_train_scaled)\n",
    "        y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "        error_list_v1[idx].extend(y_test_pred)\n",
    "        true_values_v1[idx].extend(y_test)\n",
    "\n",
    "    svr_train_models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fa5356",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(true_values_v1)):\n",
    "    plt.plot(true_values_v1[i], color=\"blue\", label = \"true data\")\n",
    "    plt.plot(error_list_v1[i], color=\"red\", label = \"prediction\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    train_mse = mean_squared_error(true_values_v1[i], error_list_v1[i])\n",
    "    train_mae =  mean_absolute_error(true_values_v1[i], error_list_v1[i])\n",
    "    sklearn_rmse = np.sqrt(train_mse)\n",
    "\n",
    "    # Create a DataFrame to display the results\n",
    "    metrics_df = pd.DataFrame({\n",
    "        \"MAE\": [train_mae],\n",
    "        \"MSE\": [train_mse],\n",
    "        \"RMSE\": [sklearn_rmse],\n",
    "    }, index=[i])\n",
    "\n",
    "    print(metrics_df, end= \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f004867",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in imfs:\n",
    "    model = CatBoostRegressor()\n",
    "    \n",
    "    for j in range(len(train_indices)):\n",
    "        df_indexes = list(train_indices[j]) + list(test_indices[j])        \n",
    "        temp_df = copy.deepcopy(df.iloc[df_indexes]).drop([\"has incorrect data\"], axis= 1)\n",
    "        train_df = copy.deepcopy(df.iloc[train_indices[j]]).drop([\"datetime\", \"grid1-loss\", \"has incorrect data\"], axis= 1)\n",
    "        test_df = copy.deepcopy(df.iloc[test_indices[j]]).drop([\"datetime\", \"grid1-loss\", \"has incorrect data\"], axis= 1)\n",
    "\n",
    "        y_train = best_decomp[i][train_indices[j]]\n",
    "        y_test = best_decomp[i][test_indices[j]]\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(train_df.values)\n",
    "        X_test_scaled = scaler.transform(test_df.values)\n",
    "\n",
    "        train_pool = Pool(X_train_scaled, y_train)\n",
    "        test_pool = Pool(X_test_scaled)\n",
    "\n",
    "        #train the model\n",
    "        model.fit(train_pool, verbose = False)\n",
    "        \n",
    "        # make the prediction using the model\n",
    "        prediction = model.predict(test_pool)\n",
    "        pred_cat_list[i].extend(prediction)\n",
    "        true_cat_list[i].extend(y_test)\n",
    "    trained_models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cb64f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(true_cat_list)):\n",
    "    plt.plot(true_cat_list[i], color=\"blue\", label = \"true data\")\n",
    "    plt.plot(pred_cat_list[i], color=\"red\", label = \"prediction\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    train_mse = mean_squared_error(true_cat_list[i], pred_cat_list[i])\n",
    "    train_mae =  mean_absolute_error(true_cat_list[i], pred_cat_list[i])\n",
    "    sklearn_rmse = np.sqrt(train_mse)\n",
    "\n",
    "    # Create a DataFrame to display the results\n",
    "    metrics_df = pd.DataFrame({\n",
    "        \"MAE\": [train_mae],\n",
    "        \"MSE\": [train_mse], # MAPE is not available in scikit-learn, so we use the custom function\n",
    "        \"RMSE\": [sklearn_rmse],\n",
    "    }, index=[i])\n",
    "\n",
    "    print(metrics_df, end= \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1709a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "super_pos = list()\n",
    "\n",
    "for i in range(len(pred_cat_list[0])):\n",
    "    sumator = 0\n",
    "    for j in range(len(pred_cat_list)):\n",
    "        sumator += pred_cat_list[j][i]\n",
    "    \n",
    "    for j in range(len(error_list_v1) - best_k):\n",
    "        sumator += error_list_v1[j][i]   \n",
    "        \n",
    "    super_pos.append(sumator)\n",
    "super_pos_array = np.array(super_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8329da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(start: int, end: int):\n",
    "    # If you have a DataFrame column, convert it to a NumPy array for comparison\n",
    "    grid1_loss_array = df[\"grid1-loss\"].iloc[start:end].reset_index(drop=True).values\n",
    "\n",
    "    # Calculate MAPE (Mean Absolute Percentage Error)\n",
    "    def mape(y_true, y_pred):\n",
    "        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "    # Calculate metrics using NumPy\n",
    "    mae = np.mean(np.abs(grid1_loss_array - super_pos_array))\n",
    "    mse = np.mean((grid1_loss_array - super_pos_array) ** 2)\n",
    "    mape_val = mape(grid1_loss_array, super_pos_array)\n",
    "\n",
    "    # Calculate metrics using scikit-learn functions\n",
    "    sklearn_mae = mean_absolute_error(grid1_loss_array, super_pos_array)\n",
    "    sklearn_mse = mean_squared_error(grid1_loss_array, super_pos_array)\n",
    "    sklearn_rmse = np.sqrt(sklearn_mse)\n",
    "    sklearn_mape = mean_absolute_percentage_error(grid1_loss_array, super_pos_array)*100\n",
    "\n",
    "    # Create a DataFrame to display the results\n",
    "    metrics_df = pd.DataFrame({\n",
    "        \"MAE\": [sklearn_mae],\n",
    "        \"MSE\": [sklearn_mse], # MAPE is not available in scikit-learn, so we use the custom function\n",
    "        \"RMSE\": [sklearn_rmse],\n",
    "        \"MAPE\": [sklearn_mape]\n",
    "    }, index=[\"sklearn\"])\n",
    "\n",
    "    plot_results(df[\"grid1-loss\"].iloc[test_indices[0][0]:test_indices[-1][-1]+1].reset_index(drop=True), super_pos_array)\n",
    "    print(metrics_df)\n",
    "\n",
    "evaluate_metrics(test_indices[0][0],test_indices[-1][-1]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e52aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition_part = list()\n",
    "\n",
    "best_ks = list()\n",
    "\n",
    "for j in range(len(ttest_indices)):\n",
    "    print(j)\n",
    "    mini = 100\n",
    "    best_k_local = 0\n",
    "    best_decom = None\n",
    "    \n",
    "    for i in range(5):\n",
    "        decom = make_eemd(df[:ttrain_indices[j][-1]+1], \"parabolic\")\n",
    "\n",
    "        for k in range(3):\n",
    "            adder = add_up(decom, k)\n",
    "            mape = evaluation_eemd(df[\"grid1-loss\"][:ttrain_indices[j][-1]+1], adder)\n",
    "            if mape < mini:\n",
    "                mini = mape\n",
    "                best_k_local = k\n",
    "                best_decom = decom\n",
    "            \n",
    "    best_ks.append(best_k_local)\n",
    "\n",
    "    print(best_k_local, mini)\n",
    "    adder = add_up(best_decom, best_k_local)\n",
    "\n",
    "    plt.plot(df[\"grid1-loss\"][:ttrain_indices[j][-1]+1], label = \"true\")\n",
    "    plt.plot(adder, label = \"IMFs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    decomposition_part.append(best_decom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ada3b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions_list_cat = [[] for _ in range(len(trained_models))]\n",
    "true_cat_pred_list = [[] for _ in range(len(trained_models))]\n",
    "\n",
    "for i, model in enumerate(trained_models):\n",
    "    for j in range(len(ttest_indices)):\n",
    "        df_indexes = list(ttrain_indices[j]) + list(ttest_indices[j])\n",
    "        print(ttrain_indices[j])\n",
    "        \n",
    "        test_df = copy.deepcopy(df.iloc[ttest_indices[j]]).drop([\"datetime\", \"grid1-loss\", \"has incorrect data\"], axis= 1)\n",
    "        train_df = copy.deepcopy(df.iloc[ttrain_indices[j]]).drop([\"datetime\", \"grid1-loss\", \"has incorrect data\"], axis= 1)\n",
    "        \n",
    "        y_train = decomposition_part[j][i][ttrain_indices[j]]\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_test_scaled = scaler.fit_transform(test_df.values)\n",
    "        X_train_scaled = scaler.transform(train_df.values)\n",
    "        \n",
    "        test_pool = Pool(X_test_scaled)\n",
    "        train_pool = Pool(X_train_scaled, y_train)\n",
    "        \n",
    "        model.fit(train_pool, verbose = False)\n",
    "\n",
    "        # make the prediction using the model\n",
    "        prediction = model.predict(test_pool)\n",
    "        predictions_list_cat[i].extend(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1adac6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(predictions_list_cat)):\n",
    "    plt.plot(predictions_list_cat[i], color=\"blue\", label = \"true data\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd839d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_list_svr = [[] for _ in range(len(svr_train_models))]\n",
    "\n",
    "for i, model in enumerate(svr_train_models):\n",
    "    for j in range(len(ttest_indices)):\n",
    "        df_indexes = list(ttrain_indices[j]) + list(ttest_indices[j])\n",
    "        print(ttrain_indices[j], ttest_indices[j])\n",
    "        \n",
    "        test_df = copy.deepcopy(df.iloc[ttest_indices[j]]).drop([\"datetime\", \"grid1-loss\", \"has incorrect data\"], axis= 1)\n",
    "        train_df = copy.deepcopy(df.iloc[ttrain_indices[j]]).drop([\"datetime\", \"grid1-loss\", \"has incorrect data\"], axis= 1)\n",
    "        y_train = decomposition_part[j][i+6][ttrain_indices[j]]\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_test_scaled = scaler.fit_transform(test_df.values)\n",
    "        X_train_scaled = scaler.transform(train_df.values)\n",
    "        \n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # make the prediction using the model\n",
    "        prediction = model.predict(X_test_scaled)\n",
    "        predictions_list_svr[i].extend(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb4a32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(predictions_list_svr)):\n",
    "    plt.plot(predictions_list_svr[i], color=\"blue\", label = \"true data\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ee5d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "super_pos1 = list()\n",
    "\n",
    "counter = 0\n",
    "for i in range(len(predictions_list_cat[0])):\n",
    "    sumator = 0\n",
    "\n",
    "    for j in range(len(predictions_list_cat)):\n",
    "        sumator += predictions_list_cat[j][i]\n",
    "    \n",
    "    for j in range(len(predictions_list_svr)):\n",
    "        sumator += predictions_list_svr[j][i]\n",
    "    \n",
    "    super_pos1.append(sumator)\n",
    "super_pos_array1 = np.array(super_pos1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0978ae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have a DataFrame column, convert it to a NumPy array for comparison\n",
    "grid1_loss_array = df[\"grid1-loss\"].iloc[ttest_indices[0][0]:ttest_indices[-1][-1]+1].reset_index(drop=True).values\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error)\n",
    "def mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# Calculate metrics using NumPy\n",
    "mae = np.mean(np.abs(grid1_loss_array - super_pos_array1))\n",
    "mse = np.mean((grid1_loss_array - super_pos_array1) ** 2)\n",
    "mape_val = mape(grid1_loss_array, super_pos_array1)\n",
    "\n",
    "\n",
    "# Calculate metrics using scikit-learn functions\n",
    "sklearn_mae = mean_absolute_error(grid1_loss_array, super_pos_array1)\n",
    "sklearn_mse = mean_squared_error(grid1_loss_array, super_pos_array1)\n",
    "sklearn_rmse = np.sqrt(sklearn_mse)\n",
    "sklearn_mape = mean_absolute_percentage_error(grid1_loss_array, super_pos_array1)*100\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"MAE\": [sklearn_mae],\n",
    "    \"MSE\": [sklearn_mse], # MAPE is not available in scikit-learn, so we use the custom function\n",
    "    \"RMSE\": [sklearn_rmse],\n",
    "    \"MAPE\": [sklearn_mape]\n",
    "}, index=[\"sklearn\"])\n",
    "\n",
    "print(metrics_df)\n",
    "plot_results(df[\"grid1-loss\"].iloc[ttest_indices[0][0]:ttest_indices[-1][-1]+1].reset_index(drop=True), super_pos_array1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T18:26:06.270076Z",
     "start_time": "2023-04-12T18:26:06.218784Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T18:39:18.369218Z",
     "start_time": "2023-04-12T18:39:18.203258Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import Pool, CatBoostRegressor\n",
    "import os\n",
    "\n",
    "# calculation\n",
    "def catboost_calc(dates, file):\n",
    "    global FILE_NAME\n",
    "    dataset = pd.read_csv(file, parse_dates=['datetime'],index_col='datetime')\n",
    "    ifs = None\n",
    "\n",
    "    for test_date in dates:\n",
    "        # create train set from all dates preceding the current test date\n",
    "        # test set is the 1 current test date\n",
    "        X_train, X_test, y_train, y_test = train_test_split(dataset[:str(test_date)].drop(['grid1-loss', 'has incorrect data'],axis=1),\n",
    "                                                            dataset[:str(test_date)][['grid1-loss']],\n",
    "                                                            test_size=len(dataset.loc[str(test_date)]), shuffle=False, stratify=None)\n",
    "\n",
    "        train_pool = Pool(X_train, y_train, cat_features=[11])\n",
    "        test_pool = Pool(X_test, cat_features=[11])\n",
    "\n",
    "        model = CatBoostRegressor()\n",
    "        #train the model\n",
    "        model.fit(train_pool, verbose=False)\n",
    "        # make the prediction using the model\n",
    "        predictions = model.predict(test_pool)\n",
    "        \n",
    "        # save importance of model features\n",
    "        if ifs is None:\n",
    "            ifs = pd.DataFrame(model.get_feature_importance(), index = pd.CategoricalIndex(train_pool.get_feature_names()), columns = [test_date]).T\n",
    "        else:\n",
    "            ifs = pd.concat([ifs, pd.DataFrame(model.get_feature_importance(), index = pd.CategoricalIndex(train_pool.get_feature_names()), columns = [test_date]).T])\n",
    "\n",
    "\n",
    "        # create forecasts dataframe and append it to file with results\n",
    "        forecasts = dataset[:str(test_date)].loc[str(test_date)][['grid1-loss']].reset_index()\n",
    "        forecasts.rename({'grid1-loss':'real'},axis=1,inplace=True)\n",
    "        forecasts['predicted'] = predictions\n",
    "        forecasts.to_csv(f'{FILE_NAME}', mode='a', index=False, header=False)\n",
    "\n",
    "    # write importance of model features\n",
    "    if os.path.isfile(f'../results/{FILE_NAME[:-4]}_ifs.csv'):\n",
    "        ifs.to_csv(f'{FILE_NAME[:-4]}_ifs.csv', mode='a', header=False)\n",
    "    else:\n",
    "        ifs.to_csv(f'{FILE_NAME[:-4]}_ifs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset with EWMA feature (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add ewma to dataset\n",
    "dataset = pd.read_csv(os.getcwd() + '/data/dataset_all.csv',parse_dates=['datetime'],index_col='datetime')\n",
    "datasets = 'SVR_dataset'\n",
    "ewma = None\n",
    "for hour in range(24):\n",
    "    data = pd.read_csv(os.getcwd() + f'/data/{datasets}/{hour:02}_00.csv',parse_dates=['datetime'],index_col='datetime').dropna()\n",
    "    ewma = data[['ewma']] if ewma is None else pd.concat([ewma, data[['ewma']]])\n",
    "dataset['ewma'] = ewma\n",
    "dataset.to_csv(os.getcwd() +'/data/dataset_all_ewma.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset with curve features (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add curve features\n",
    "dataset = pd.read_csv(os.getcwd() + '/data/dataset_all_ewma.csv',parse_dates=['datetime'],index_col='datetime')\n",
    "\n",
    "# weather_columns = [\"temperature\", \"dew_point\", \"grid1-temp\"]\n",
    "weather_columns = ['grid1-load', 'grid1-temp']\n",
    "w = 24\n",
    "\n",
    "for column in weather_columns:\n",
    "    dataset = pd.concat([dataset,dataset[column].rolling(w).agg(['mean', 'min', 'max']).add_prefix(f'{column}_')], axis=1)\n",
    "    dataset[f'{column}_mean_diff'] = dataset[column].diff().rolling(w).mean()\n",
    "dataset.dropna().to_csv(os.getcwd() + '/data/dataset_all_cfsall_wow.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T18:41:29.053678Z",
     "start_time": "2023-04-12T18:41:28.918277Z"
    }
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "FILE_NAME = 'CatBoost_cfsall_wow.csv' # name of file with results\n",
    "dataset_file = os.getcwd() + '/data/dataset_all_cfsall_wow.csv' # file with dataset\n",
    "\n",
    "# create empty file for results with header\n",
    "f = open(f'{FILE_NAME}', \"w\")\n",
    "f.write(\"datetime,real,predicted\\n\")\n",
    "f.close()\n",
    "\n",
    "# division of dates into 8 groups for parallelization\n",
    "data = pd.read_csv(dataset_file, parse_dates=['datetime'],index_col='datetime')\n",
    "dates = data.resample('D').count()[['grid1-loss']]\n",
    "dates = dates[dates['grid1-loss']>0]\n",
    "DATES = dates.loc['12-2019':].reset_index()['datetime'].apply(datetime.date).to_frame()['datetime']\n",
    "dates_l = list(DATES)\n",
    "dates_groups = np.array_split(dates_l, 8)\n",
    "\n",
    "# run 8 parallel processes\n",
    "for dates in dates_groups:\n",
    "    p = Process(target = catboost_calc, args = (dates,dataset_file))\n",
    "    p.start()\n",
    "    time.sleep(5)"
   ]
  },
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
